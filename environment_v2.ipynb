{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from random import *\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import RMSprop\n",
    "from collections import deque\n",
    "from tqdm import tqdm\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = ['6633', '6024', '1062', '6310', '6128', '6287', '1075', '1076',\n",
    "       '6756', '6757', '1101', '6494', '6662', '6705', '6549', '6552',\n",
    "       '6527', '6531', '1099', '1104', '6754', '6575', '6706', '6589',\n",
    "       '6232', '6752', '6574', '6053', 'P828', '6692', '6721', '6663',\n",
    "       '6734', '6681', '6682', '6561', '6750', '6728', '6648', '6584',\n",
    "       '6102', '6630', '6406', '6407', '1106', '1107', '6542', '6723',\n",
    "       '6356', '6700', '6572', '6573', '6218', '1017', 'P130', '6763',\n",
    "       '6650', '6653', '6454', '6452', '6696', '8L39', '8L72', '6698',\n",
    "       '6433', '6629', '6760', '6380', 'P099', '6585', '6581', '6447',\n",
    "       '6689', '6761', '6040', '6515', '6217', '6747', '6769', '6738',\n",
    "       '6751', '6112', '6753', '6301', '6271', '6432', '6675', '6470',\n",
    "       '6647', '1018', '6238', '6324', '6733', '6206', 'P163', '6719',\n",
    "       '6205', '6660', '6453', '6655', '6646', '6244', '6695', '6539',\n",
    "       '6736', '6683', '6634', '6475', 'P147', '6642', '6684', '6477',\n",
    "       '6608', '6708', 'P837', 'P820', 'P835', '8K52']\n",
    "machine = [[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]]\n",
    "machine_state = [[0,0,0,0,0],[0,0,0,0,0],[0,0,0,0,0],[0,0,0,0,0],[0,0,0,0,0],[0,0,0,0,0],[0,0,0,0,0],[0,0,0,0,0],[0,0,0,0,0],[0,0,0,0,0],[0,0,0,0,0],[0,0,0,0,0],[0,0,0,0,0],[0,0,0,0,0],[0,0,0,0,0],[0,0,0,0,0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.read_csv(\"C:\\\\eric\\\\Projects\\\\APS_Betha\\\\Codes\\\\datasets\\\\Sch\\\\data_scheduling.csv\")\n",
    "#           ORDER_ID type  process_time  due_date      \n",
    "# 0    20171221-2257  6633            10        10      \n",
    "# 1    20171221-1374  6024             6        10      \n",
    "# 2    20171221-1348  1062             3        10      \n",
    "# 3    20171221-1315  6310             2        10      \n",
    "# 4    20171221-1313  6310             5        10 \n",
    "\n",
    "len(new_df['type'].unique())\n",
    "\n",
    "new_table = pd.read_csv(\"C:\\\\eric\\\\Projects\\\\APS_Betha\\\\Codes\\\\datasets\\\\Sch\\\\data_const.csv\")\n",
    "#      machine  type\n",
    "# 0      12216  1075\n",
    "# 1      12216  1076\n",
    "# 2      12216  1099\n",
    "# 3      12216  1101\n",
    "# 4      12216  1104\n",
    "\n",
    "#------------------------------------------------------------------------------------------------#\n",
    "# new_table01 = 새로운 const df \n",
    "new_table01 = new_table.groupby('machine').agg(lambda x: ','.join(set(x)))\n",
    "new_table01 = new_table01.reset_index()\n",
    "new_table01 = new_table01.reset_index()\n",
    "del new_table01['machine']\n",
    "new_table01.columns = ['machine','type']\n",
    "new_table01\n",
    "for i in range(0,16):\n",
    "    new_table01['type'][i] = new_table01['type'][i].split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "class CustomHistory(keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        self.train_loss = []\n",
    "        self.val_loss = []\n",
    "        self.train_acc = []\n",
    "        self.val_acc = []\n",
    "        \n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.train_loss.append(logs.get('loss'))\n",
    "        self.val_loss.append(logs.get('val_loss'))\n",
    "        self.train_acc.append(logs.get('acc'))\n",
    "        self.val_acc.append(logs.get('val_acc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "custom_hist = CustomHistory()\n",
    "custom_hist.__init__()\n",
    "\n",
    "class DQN:\n",
    "    def __init__(self):\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        \n",
    "        self.gamma = 0.85\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.005\n",
    "        self.tau = .125\n",
    "        \n",
    "        self.model = self.create_model()\n",
    "        self.target_model = self.create_model()\n",
    "        \n",
    "    def create_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(64, input_dim=198, activation=\"relu\"))\n",
    "        model.add(Dense(32, activation=\"relu\"))\n",
    "        model.add(Dense(16, activation=\"relu\"))\n",
    "        model.add(Dense(118))\n",
    "        model.compile(loss=\"mean_squared_error\", optimizer=Adam(lr=self.learning_rate))\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def act(self, cur_state):\n",
    "        self.epsilon *= self.epsilon_decay\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon)\n",
    "        random_val = np.random.random()\n",
    "\n",
    "        if random_val < self.epsilon:\n",
    "            blank = []\n",
    "            for i in range(len(new_table01['type'])):\n",
    "                for j in range(len(new_table01['type'][i])):\n",
    "                    if new_table01['type'][i][j] == c_type:\n",
    "                        blank.append(i)\n",
    "            return random.choice(blank)\n",
    "\n",
    "        else:\n",
    "            blank = []\n",
    "            for i in range(len(new_table01['type'])):\n",
    "                for j in range(len(new_table01['type'][i])):\n",
    "                    if new_table01['type'][i][j] == c_type:\n",
    "                        blank.append(i)\n",
    "            score = self.model.predict(state)\n",
    "\n",
    "            blank2 = []\n",
    "            for i in blank:\n",
    "                ax = score[0][i]\n",
    "                blank2.append(ax)\n",
    "            max_q_val = max(blank2)\n",
    "\n",
    "            index_of_max = np.where(score[0] == max_q_val)\n",
    "            iom = index_of_max[0][0]\n",
    "            # np.argmax(self.model.predict(state)[0])\n",
    "            return iom\n",
    "\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append([state, action, reward, next_state, done])\n",
    "        \n",
    "    def replay(self):\n",
    "        batch_size = 36\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        \n",
    "        samples = random.sample(self.memory, batch_size)\n",
    "        X=[]\n",
    "        Y=[]\n",
    "        for sample in samples:\n",
    "            state, action, reward, next_state, done = sample\n",
    "            target = self.target_model.predict(state)\n",
    "            if done:\n",
    "                target[0][action] = reward\n",
    "            else:\n",
    "                Q_future = max(self.target_model.predict(next_state)[0])\n",
    "                target[0][action] = reward + Q_future * self.gamma\n",
    "            X.append(state)\n",
    "            Y.append(target)\n",
    "        X = np.array(X)\n",
    "        Y = np.array(Y)    \n",
    "        self.model.fit(X, Y, epochs=1, batch_size=64 ,verbose=0, callbacks=[custom_hist])\n",
    "\n",
    "    def target_train(self):\n",
    "        weights = self.model.get_weights()\n",
    "        target_weights = self.target_model.get_weights()\n",
    "        for i in range(len(target_weights)):\n",
    "            target_weights[i] = weights[i]* self.tau + target_weights[i]*(1-self.tau)\n",
    "        self.target_model.set_weights(target_weights)\n",
    "        \n",
    "    def save_model(self, fn):\n",
    "        self.model.save(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_agent = DQN()\n",
    "\n",
    "job_change_count = 0\n",
    "validation_date_count = 0\n",
    "num_games = 500\n",
    "done = False\n",
    "\n",
    "\n",
    "for game in tqdm(range( 0, num_games )):\n",
    "    \n",
    "    done = False\n",
    "\n",
    "    current = np.array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0.]])\n",
    "    current = current.flatten()\n",
    "    current = current.tolist()\n",
    "\n",
    "    machine_state_np = np.array(machine_state)\n",
    "    machine_state_flatten = machine_state_np.flatten()\n",
    "    # machine_state\n",
    "    machine_state_final = machine_state_flatten.tolist()\n",
    "    # next machine_state\n",
    "    state = current + machine_state_final\n",
    "    state = np.array(state)\n",
    "    state = state.reshape(1,198)\n",
    "\n",
    "    reward_total =0 \n",
    "    validation_date_count = 0\n",
    "    job_change_count = 0\n",
    "\n",
    "    action_list = [] \n",
    "    for j in range(1,len(new_df)): # order list\n",
    "        #state, action, reward, next_state, done \n",
    "        if j == len(new_df)-2:\n",
    "            done = True\n",
    "\n",
    "        # order check \n",
    "        c_type = new_df['type'][j]  \n",
    "        c_process_time = new_df['process_time'][j]\n",
    "        c_due = new_df['due_date'][j]\n",
    "\n",
    "        #c_type encoding \n",
    "        current = np.identity(118)[types.index(c_type):types.index(c_type)+1]\n",
    "        current = current.flatten()\n",
    "        current = current.tolist()\n",
    "\n",
    "        #action\n",
    "        action = dqn_agent.act(state)\n",
    "        #print(action) \n",
    "        action_list.append(action)  \n",
    "\n",
    "        # reward\n",
    "        if len(machine[action]) == 0 or machine[action][-1] == c_type:\n",
    "            job_change = 0\n",
    "        else:\n",
    "            job_change = 1\n",
    "            job_change_count += 1\n",
    "\n",
    "\n",
    "        if machine_state[action][4] < 0:\n",
    "            validation_due_date = machine_state[action][4]\n",
    "            validation_due_date = -1 * validation_due_date\n",
    "            validation_date_count += validation_due_date\n",
    "        else:\n",
    "            validation_due_date = 0\n",
    "        \n",
    "\n",
    "        #if machine_state[action][4] < 0:\n",
    "        #    validation_due_date = machine_state[action][4]\n",
    "        #    validation_date_count += 1\n",
    "        #else:\n",
    "        #    validation_due_date = 0\n",
    "\n",
    "        reward = 50000 + (-1*job_change) + (-5*validation_due_date)\n",
    "        reward_total += reward\n",
    "\n",
    "\n",
    "        #env changed \n",
    "        ####\n",
    "        for j in range(c_process_time):\n",
    "            machine[action].append(c_type)\n",
    "\n",
    "\n",
    "        #status update\n",
    "\n",
    "        # 0 #status of machine #processed slots \n",
    "        for k in range(0, len(machine)): \n",
    "            machine_len = len(machine[k])\n",
    "            machine_state[k][0] = machine_len\n",
    "\n",
    "        # 1 #status of order #number of order \n",
    "        for k in range(0, len(machine)):\n",
    "            machine_state[k][1] = c_process_time \n",
    "\n",
    "        # 2 #status of order #due date of order\n",
    "        for k in range(0, len(machine)):\n",
    "            machine_state[k][2] = c_due\n",
    "        \n",
    "        # 3 #compare between state and order # same block indicator \n",
    "        for k in range(0, len(machine)):\n",
    "            if len(machine[k]) ==0 or machine[k][-1] == c_type:\n",
    "                machine_state[k][3] = 1\n",
    "            else:\n",
    "                machine_state[k][3] = 0    \n",
    "\n",
    "        # 4 #compare between state and order # due date violation \n",
    "        for k in range(0, len(machine)):\n",
    "            machine_state[k][4] = c_due - (len(machine[k]) + c_process_time) \n",
    "        \n",
    "        #print(machine_state)\n",
    "\n",
    "\n",
    "\n",
    "        machine_state_np = np.array(machine_state)\n",
    "        machine_state_flatten = machine_state_np.flatten()\n",
    "        # machine_state\n",
    "        machine_state_final = machine_state_flatten.tolist()\n",
    "        # next machine_state\n",
    "        next_state = current + machine_state_final\n",
    "        next_state = np.array(next_state)\n",
    "        next_state = next_state.reshape(1,198)\n",
    "        ##################################################################\n",
    "        #state, action, reward, next_state, done 저장 및 학습\n",
    "\n",
    "\n",
    "        dqn_agent.remember(state, action, reward, next_state, done)\n",
    "\n",
    "        #dqn_agent.replay()\n",
    "        #dqn_agent.target_train()\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "        #dqn_agent.save_model(\"success.model\") # 여기도,, 적어도 episode 몇백번은 반복을 하고 나서 model save 를 해야지.. \n",
    "        #print(machine)\n",
    "        #print(c_type)\n",
    "    dqn_agent.replay()\n",
    "    dqn_agent.target_train()\n",
    "    dqn_agent.save_model(\"success.model\") # 여기도,, 적어도 episode 몇백번은 반복을 하고 나서 model save 를 해야지.. \n",
    "    \n",
    "    print(\"job_change_count = \" + str(job_change_count) + \", violation_date_count = \" + str(validation_date_count))    \n",
    "    print(reward_total)\n",
    "    print(action_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "print(custom_hist)\n",
    "fig, loss_ax = plt.subplots()\n",
    "\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(custom_hist.train_loss, 'y', label='train loss')\n",
    "loss_ax.plot(custom_hist.val_loss, 'r', label='val loss')\n",
    "\n",
    "acc_ax.plot(custom_hist.train_acc, 'b', label='train acc')\n",
    "acc_ax.plot(custom_hist.val_acc, 'g', label='val acc')\n",
    "\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "acc_ax.set_ylabel('accuray')\n",
    "\n",
    "loss_ax.legend(loc='upper left')\n",
    "acc_ax.legend(loc='lower left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_agent.save_model(\"model.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
